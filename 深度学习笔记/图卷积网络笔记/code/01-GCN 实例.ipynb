{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:19.518283Z",
     "start_time": "2021-02-27T12:28:13.909606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(indices=tensor([[   0,    8,   14,  ..., 1389, 2344, 2707],\n",
       "                        [   0,    0,    0,  ..., 2707, 2707, 2707]]),\n",
       "        values=tensor([0.1667, 0.1667, 0.0500,  ..., 0.2000, 0.5000, 0.2500]),\n",
       "        size=(2708, 2708), nnz=13264, layout=torch.sparse_coo),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([0, 4, 5,  ..., 3, 6, 0]),\n",
       " tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139]),\n",
       " tensor([200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213,\n",
       "         214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
       "         228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241,\n",
       "         242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,\n",
       "         256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269,\n",
       "         270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283,\n",
       "         284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297,\n",
       "         298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "         312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
       "         326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339,\n",
       "         340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
       "         354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
       "         368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
       "         382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
       "         396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409,\n",
       "         410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423,\n",
       "         424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437,\n",
       "         438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451,\n",
       "         452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "         466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479,\n",
       "         480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "         494, 495, 496, 497, 498, 499]),\n",
       " tensor([ 500,  501,  502,  503,  504,  505,  506,  507,  508,  509,  510,  511,\n",
       "          512,  513,  514,  515,  516,  517,  518,  519,  520,  521,  522,  523,\n",
       "          524,  525,  526,  527,  528,  529,  530,  531,  532,  533,  534,  535,\n",
       "          536,  537,  538,  539,  540,  541,  542,  543,  544,  545,  546,  547,\n",
       "          548,  549,  550,  551,  552,  553,  554,  555,  556,  557,  558,  559,\n",
       "          560,  561,  562,  563,  564,  565,  566,  567,  568,  569,  570,  571,\n",
       "          572,  573,  574,  575,  576,  577,  578,  579,  580,  581,  582,  583,\n",
       "          584,  585,  586,  587,  588,  589,  590,  591,  592,  593,  594,  595,\n",
       "          596,  597,  598,  599,  600,  601,  602,  603,  604,  605,  606,  607,\n",
       "          608,  609,  610,  611,  612,  613,  614,  615,  616,  617,  618,  619,\n",
       "          620,  621,  622,  623,  624,  625,  626,  627,  628,  629,  630,  631,\n",
       "          632,  633,  634,  635,  636,  637,  638,  639,  640,  641,  642,  643,\n",
       "          644,  645,  646,  647,  648,  649,  650,  651,  652,  653,  654,  655,\n",
       "          656,  657,  658,  659,  660,  661,  662,  663,  664,  665,  666,  667,\n",
       "          668,  669,  670,  671,  672,  673,  674,  675,  676,  677,  678,  679,\n",
       "          680,  681,  682,  683,  684,  685,  686,  687,  688,  689,  690,  691,\n",
       "          692,  693,  694,  695,  696,  697,  698,  699,  700,  701,  702,  703,\n",
       "          704,  705,  706,  707,  708,  709,  710,  711,  712,  713,  714,  715,\n",
       "          716,  717,  718,  719,  720,  721,  722,  723,  724,  725,  726,  727,\n",
       "          728,  729,  730,  731,  732,  733,  734,  735,  736,  737,  738,  739,\n",
       "          740,  741,  742,  743,  744,  745,  746,  747,  748,  749,  750,  751,\n",
       "          752,  753,  754,  755,  756,  757,  758,  759,  760,  761,  762,  763,\n",
       "          764,  765,  766,  767,  768,  769,  770,  771,  772,  773,  774,  775,\n",
       "          776,  777,  778,  779,  780,  781,  782,  783,  784,  785,  786,  787,\n",
       "          788,  789,  790,  791,  792,  793,  794,  795,  796,  797,  798,  799,\n",
       "          800,  801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
       "          812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,  823,\n",
       "          824,  825,  826,  827,  828,  829,  830,  831,  832,  833,  834,  835,\n",
       "          836,  837,  838,  839,  840,  841,  842,  843,  844,  845,  846,  847,\n",
       "          848,  849,  850,  851,  852,  853,  854,  855,  856,  857,  858,  859,\n",
       "          860,  861,  862,  863,  864,  865,  866,  867,  868,  869,  870,  871,\n",
       "          872,  873,  874,  875,  876,  877,  878,  879,  880,  881,  882,  883,\n",
       "          884,  885,  886,  887,  888,  889,  890,  891,  892,  893,  894,  895,\n",
       "          896,  897,  898,  899,  900,  901,  902,  903,  904,  905,  906,  907,\n",
       "          908,  909,  910,  911,  912,  913,  914,  915,  916,  917,  918,  919,\n",
       "          920,  921,  922,  923,  924,  925,  926,  927,  928,  929,  930,  931,\n",
       "          932,  933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
       "          944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,  955,\n",
       "          956,  957,  958,  959,  960,  961,  962,  963,  964,  965,  966,  967,\n",
       "          968,  969,  970,  971,  972,  973,  974,  975,  976,  977,  978,  979,\n",
       "          980,  981,  982,  983,  984,  985,  986,  987,  988,  989,  990,  991,\n",
       "          992,  993,  994,  995,  996,  997,  998,  999, 1000, 1001, 1002, 1003,\n",
       "         1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015,\n",
       "         1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027,\n",
       "         1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039,\n",
       "         1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051,\n",
       "         1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063,\n",
       "         1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075,\n",
       "         1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087,\n",
       "         1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099,\n",
       "         1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111,\n",
       "         1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123,\n",
       "         1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135,\n",
       "         1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147,\n",
       "         1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159,\n",
       "         1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171,\n",
       "         1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183,\n",
       "         1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195,\n",
       "         1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207,\n",
       "         1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219,\n",
       "         1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231,\n",
       "         1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243,\n",
       "         1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255,\n",
       "         1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267,\n",
       "         1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279,\n",
       "         1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291,\n",
       "         1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303,\n",
       "         1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315,\n",
       "         1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327,\n",
       "         1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339,\n",
       "         1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351,\n",
       "         1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363,\n",
       "         1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375,\n",
       "         1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387,\n",
       "         1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399,\n",
       "         1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411,\n",
       "         1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423,\n",
       "         1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435,\n",
       "         1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447,\n",
       "         1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459,\n",
       "         1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471,\n",
       "         1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483,\n",
       "         1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495,\n",
       "         1496, 1497, 1498, 1499]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据预处理\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    \"\"\"\n",
    "    将类别表示为one-hot形式的矩阵表示\n",
    "    \"\"\"\n",
    "    classes = set(labels)\n",
    "    classes_dict = {\n",
    "        c:np.identity(len(classes))[i,:] for i,c in enumerate(classes)\n",
    "    }\n",
    "    \"\"\"\n",
    "    >>> a = {\"a\":1,\"b\":2}\n",
    "    >>> labels = ['a','b','a','a']\n",
    "    >>> list(map(a.get,labels))\n",
    "    [1, 2, 1, 1]\n",
    "    \"\"\"\n",
    "    # 标签类别转换位onehot形式\n",
    "    labels_onehot = np.array(list(map(classes_dict.get,labels)),dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"\n",
    "    首先对每一行求和得到rowsum；求倒数得到r_inv；\n",
    "    如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0；\n",
    "    构建对角元素为r_inv的对角的元素；\n",
    "    用对角矩阵与原始矩阵的点积起到标准化的作用，原始矩阵中每一行元素都会与对应的r_inv相乘。\n",
    "    \"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "  \n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "# 加载数据\n",
    "def load_data(path, dataset):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "    \n",
    "    # 邻接矩阵；特征；类别；训练集合测试集\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "load_data(path=\"./data/cora/\",dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:19.529993Z",
     "start_time": "2021-02-27T12:28:19.520235Z"
    }
   },
   "outputs": [],
   "source": [
    "# 图卷积\n",
    "import torch,math\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class GraphConvolution(torch.nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    GCN Layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,in_features,out_features,bias=True):\n",
    "        super(GraphConvolution,self).__init__()\n",
    "        self.in_featues = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        \"\"\"\n",
    "        torch.nn.parameter.Parameter(torch.FloatTensor(hidden_size))：\n",
    "        Parameter是Tensor，即 Tensor 拥有的属性它都有，⽐如可以根据data 来访问参数数值，⽤ grad 来访问参数梯度。\n",
    "       \n",
    "       \n",
    "        register_parameter：向建立的网络module添加 parameter；最大的区别：parameter可以通过注册网络时候的name获取。\n",
    "       \n",
    "       \"\"\"\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features,out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias',None)\n",
    "            \n",
    "        # 初始化参数\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self,):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        # 使用均匀分布来初始化其权重\n",
    "        self.weight.data.uniform_(-stdv,stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv,stdv)\n",
    "            \n",
    "    def forward(self,input,adj):\n",
    "        # AXW\n",
    "        ## XW \n",
    "        support = torch.mm(input,self.weight) # torch.mm：矩阵相乘\n",
    "        ## A(XW)\n",
    "        output = torch.spmm(adj,support) # 稀疏矩阵相乘\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "    def __repr(self,):\n",
    "        return \"{}({}->{})\".format(self.__class__.__name__,str(self.in_features),str(self.out_features)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:19.546219Z",
     "start_time": "2021-02-27T12:28:19.534969Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,nfeat,nhid,nclass,dropout):\n",
    "        \"\"\"\n",
    "        nfeat：特征数\n",
    "        nhid：隐藏层大小\n",
    "        nclass：目标维度\n",
    "        \"\"\"\n",
    "        super(GCN,self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat,nhid)\n",
    "        self.gc2 = GraphConvolution(nhid,nclass)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        \n",
    "    def forward(self,x,adj):\n",
    "        x = F.relu(self.gc1(x,adj))\n",
    "        # 带Dropout的网络可以防止出现过拟合。\n",
    "        x = F.dropout(x,self.dropout,training=self.training)\n",
    "        # 列的和为1\n",
    "        x = F.log_softmax(self.gc2(x,adj),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:19.552074Z",
     "start_time": "2021-02-27T12:28:19.548172Z"
    }
   },
   "outputs": [],
   "source": [
    "# 准确率\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:19.612586Z",
     "start_time": "2021-02-27T12:28:19.554028Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(seed=42,\n",
       "          no_cuda=False,\n",
       "          fastmode=False,\n",
       "          epochs=200,\n",
       "          lr=0.01,\n",
       "          weight_decay=0.0005,\n",
       "          hidden=16,\n",
       "          dropout=0.5,\n",
       "          cuda=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training settings\n",
    "\n",
    "from types import SimpleNamespace\n",
    "import torch\n",
    "\n",
    "args = {\n",
    "    'seed':42,\n",
    "    'no_cuda':False,\n",
    "    'fastmode':False, # Validate during training pass.\n",
    "    'epochs':200, # 步长\n",
    "    'lr':0.01, # 学习率\n",
    "    'weight_decay':5e-4, # 权重衰减（L2惩罚）（默认: 0）\n",
    "    'hidden':16, # 隐藏层\n",
    "    'dropout':0.5,\n",
    "}\n",
    "# 将字典转换为对象\n",
    "args = SimpleNamespace(**args)\n",
    "# 检查cuda是否可用\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:19.625273Z",
     "start_time": "2021-02-27T12:28:19.614537Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed) #为CPU设置种子用于生成随机数，以使得结果是确定的\n",
    "if args.cuda: #为当前GPU设置随机种子；如果使用多个GPU，应该使用torch.cuda.manual_seed_all()为所有的GPU设置种子。\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:24.438366Z",
     "start_time": "2021-02-27T12:28:19.628201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(path=\"./data/cora/\",dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:24.445199Z",
     "start_time": "2021-02-27T12:28:24.439342Z"
    }
   },
   "outputs": [],
   "source": [
    "# 模型和优化器\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,nclass=labels.max().item()+1,\n",
    "            dropout=args.dropout)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                      lr=args.lr,weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:25.862839Z",
     "start_time": "2021-02-27T12:28:24.446174Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.cuda: # 对model自身进行的内存迁移\n",
    "    \"\"\"\n",
    "    model = model.cuda() \n",
    "    <=>\n",
    "    model.cuda() \n",
    "    \"\"\"\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:25.870648Z",
     "start_time": "2021-02-27T12:28:25.863816Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as F\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    \"\"\"\n",
    "    model.eval()，pytorch会自动把BN和DropOut固定住，不会取平均，而是用训练好的值。不然的话，一旦test的batch_size过小，很容易就会被BN层导致生成图片颜色失真极大；在模型测试阶段使用\n",
    "\n",
    "    model.train() 让model变成训练模式，此时 dropout和batch normalization的操作在训练q起到防止网络过拟合的问题\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features,adj)\n",
    "    loss_train = F.nll_loss(output[idx_train],labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train],labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "    \n",
    "    loss_val = F.nll_loss(output[idx_val],labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val],labels[idx_val])\n",
    "    \n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:28.263043Z",
     "start_time": "2021-02-27T12:28:25.872601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9926 acc_train: 0.1357 loss_val: 1.9962 acc_val: 0.1567 time: 0.8208s\n",
      "Epoch: 0002 loss_train: 1.9796 acc_train: 0.1857 loss_val: 1.9841 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0003 loss_train: 1.9645 acc_train: 0.2071 loss_val: 1.9729 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0004 loss_train: 1.9584 acc_train: 0.2000 loss_val: 1.9620 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0005 loss_train: 1.9488 acc_train: 0.2000 loss_val: 1.9513 acc_val: 0.1567 time: 0.0078s\n",
      "Epoch: 0006 loss_train: 1.9324 acc_train: 0.2000 loss_val: 1.9408 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0007 loss_train: 1.9135 acc_train: 0.2000 loss_val: 1.9305 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0008 loss_train: 1.9082 acc_train: 0.2000 loss_val: 1.9202 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0009 loss_train: 1.9026 acc_train: 0.2000 loss_val: 1.9096 acc_val: 0.1567 time: 0.0078s\n",
      "Epoch: 0010 loss_train: 1.8910 acc_train: 0.2000 loss_val: 1.8990 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0011 loss_train: 1.8782 acc_train: 0.2000 loss_val: 1.8883 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0012 loss_train: 1.8679 acc_train: 0.2000 loss_val: 1.8773 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0013 loss_train: 1.8521 acc_train: 0.2071 loss_val: 1.8662 acc_val: 0.1567 time: 0.0107s\n",
      "Epoch: 0014 loss_train: 1.8466 acc_train: 0.2071 loss_val: 1.8552 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0015 loss_train: 1.8433 acc_train: 0.2143 loss_val: 1.8442 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0016 loss_train: 1.8417 acc_train: 0.2214 loss_val: 1.8335 acc_val: 0.1567 time: 0.0068s\n",
      "Epoch: 0017 loss_train: 1.8098 acc_train: 0.2286 loss_val: 1.8228 acc_val: 0.1567 time: 0.0059s\n",
      "Epoch: 0018 loss_train: 1.7931 acc_train: 0.3000 loss_val: 1.8116 acc_val: 0.2067 time: 0.0068s\n",
      "Epoch: 0019 loss_train: 1.7906 acc_train: 0.3500 loss_val: 1.8004 acc_val: 0.4133 time: 0.0068s\n",
      "Epoch: 0020 loss_train: 1.8102 acc_train: 0.2643 loss_val: 1.7894 acc_val: 0.4633 time: 0.0068s\n",
      "Epoch: 0021 loss_train: 1.7531 acc_train: 0.3714 loss_val: 1.7786 acc_val: 0.4500 time: 0.0068s\n",
      "Epoch: 0022 loss_train: 1.7748 acc_train: 0.3429 loss_val: 1.7682 acc_val: 0.3933 time: 0.0068s\n",
      "Epoch: 0023 loss_train: 1.7499 acc_train: 0.3500 loss_val: 1.7582 acc_val: 0.3733 time: 0.0068s\n",
      "Epoch: 0024 loss_train: 1.7440 acc_train: 0.3714 loss_val: 1.7485 acc_val: 0.3667 time: 0.0078s\n",
      "Epoch: 0025 loss_train: 1.7321 acc_train: 0.3214 loss_val: 1.7393 acc_val: 0.3633 time: 0.0068s\n",
      "Epoch: 0026 loss_train: 1.7039 acc_train: 0.3857 loss_val: 1.7302 acc_val: 0.3633 time: 0.0068s\n",
      "Epoch: 0027 loss_train: 1.7196 acc_train: 0.3429 loss_val: 1.7214 acc_val: 0.3600 time: 0.0068s\n",
      "Epoch: 0028 loss_train: 1.7073 acc_train: 0.3643 loss_val: 1.7130 acc_val: 0.3567 time: 0.0078s\n",
      "Epoch: 0029 loss_train: 1.6872 acc_train: 0.3571 loss_val: 1.7047 acc_val: 0.3567 time: 0.0068s\n",
      "Epoch: 0030 loss_train: 1.6931 acc_train: 0.3286 loss_val: 1.6966 acc_val: 0.3600 time: 0.0068s\n",
      "Epoch: 0031 loss_train: 1.6844 acc_train: 0.3714 loss_val: 1.6884 acc_val: 0.3633 time: 0.0068s\n",
      "Epoch: 0032 loss_train: 1.6597 acc_train: 0.3571 loss_val: 1.6803 acc_val: 0.3633 time: 0.0068s\n",
      "Epoch: 0033 loss_train: 1.6595 acc_train: 0.3929 loss_val: 1.6722 acc_val: 0.3633 time: 0.0068s\n",
      "Epoch: 0034 loss_train: 1.6285 acc_train: 0.3857 loss_val: 1.6642 acc_val: 0.3667 time: 0.0078s\n",
      "Epoch: 0035 loss_train: 1.6133 acc_train: 0.3500 loss_val: 1.6560 acc_val: 0.3667 time: 0.0068s\n",
      "Epoch: 0036 loss_train: 1.6421 acc_train: 0.3929 loss_val: 1.6479 acc_val: 0.3700 time: 0.0078s\n",
      "Epoch: 0037 loss_train: 1.6044 acc_train: 0.3929 loss_val: 1.6397 acc_val: 0.3733 time: 0.0068s\n",
      "Epoch: 0038 loss_train: 1.6114 acc_train: 0.3929 loss_val: 1.6314 acc_val: 0.3867 time: 0.0068s\n",
      "Epoch: 0039 loss_train: 1.5744 acc_train: 0.4000 loss_val: 1.6228 acc_val: 0.3967 time: 0.0068s\n",
      "Epoch: 0040 loss_train: 1.5467 acc_train: 0.4500 loss_val: 1.6138 acc_val: 0.4067 time: 0.0068s\n",
      "Epoch: 0041 loss_train: 1.5442 acc_train: 0.4286 loss_val: 1.6046 acc_val: 0.4100 time: 0.0059s\n",
      "Epoch: 0042 loss_train: 1.5129 acc_train: 0.4571 loss_val: 1.5951 acc_val: 0.4367 time: 0.0059s\n",
      "Epoch: 0043 loss_train: 1.5383 acc_train: 0.4714 loss_val: 1.5852 acc_val: 0.4700 time: 0.0068s\n",
      "Epoch: 0044 loss_train: 1.5126 acc_train: 0.4500 loss_val: 1.5751 acc_val: 0.4833 time: 0.0068s\n",
      "Epoch: 0045 loss_train: 1.4640 acc_train: 0.5143 loss_val: 1.5644 acc_val: 0.5000 time: 0.0059s\n",
      "Epoch: 0046 loss_train: 1.4583 acc_train: 0.5214 loss_val: 1.5534 acc_val: 0.5167 time: 0.0068s\n",
      "Epoch: 0047 loss_train: 1.4575 acc_train: 0.5143 loss_val: 1.5421 acc_val: 0.5267 time: 0.0068s\n",
      "Epoch: 0048 loss_train: 1.4453 acc_train: 0.5286 loss_val: 1.5305 acc_val: 0.5367 time: 0.0068s\n",
      "Epoch: 0049 loss_train: 1.4392 acc_train: 0.5714 loss_val: 1.5185 acc_val: 0.5433 time: 0.0068s\n",
      "Epoch: 0050 loss_train: 1.4161 acc_train: 0.5500 loss_val: 1.5066 acc_val: 0.5500 time: 0.0098s\n",
      "Epoch: 0051 loss_train: 1.3964 acc_train: 0.5857 loss_val: 1.4944 acc_val: 0.5500 time: 0.0068s\n",
      "Epoch: 0052 loss_train: 1.3840 acc_train: 0.6071 loss_val: 1.4822 acc_val: 0.5600 time: 0.0068s\n",
      "Epoch: 0053 loss_train: 1.3410 acc_train: 0.5643 loss_val: 1.4699 acc_val: 0.5800 time: 0.0069s\n",
      "Epoch: 0054 loss_train: 1.3454 acc_train: 0.5714 loss_val: 1.4572 acc_val: 0.5833 time: 0.0068s\n",
      "Epoch: 0055 loss_train: 1.3651 acc_train: 0.5929 loss_val: 1.4444 acc_val: 0.5900 time: 0.0078s\n",
      "Epoch: 0056 loss_train: 1.2895 acc_train: 0.6429 loss_val: 1.4314 acc_val: 0.5967 time: 0.0059s\n",
      "Epoch: 0057 loss_train: 1.2762 acc_train: 0.6500 loss_val: 1.4185 acc_val: 0.6067 time: 0.0068s\n",
      "Epoch: 0058 loss_train: 1.2760 acc_train: 0.6357 loss_val: 1.4057 acc_val: 0.6167 time: 0.0068s\n",
      "Epoch: 0059 loss_train: 1.2818 acc_train: 0.6286 loss_val: 1.3928 acc_val: 0.6233 time: 0.0068s\n",
      "Epoch: 0060 loss_train: 1.2062 acc_train: 0.6714 loss_val: 1.3804 acc_val: 0.6333 time: 0.0078s\n",
      "Epoch: 0061 loss_train: 1.2713 acc_train: 0.6286 loss_val: 1.3676 acc_val: 0.6400 time: 0.0059s\n",
      "Epoch: 0062 loss_train: 1.2544 acc_train: 0.6786 loss_val: 1.3550 acc_val: 0.6400 time: 0.0068s\n",
      "Epoch: 0063 loss_train: 1.2005 acc_train: 0.6786 loss_val: 1.3426 acc_val: 0.6500 time: 0.0059s\n",
      "Epoch: 0064 loss_train: 1.2020 acc_train: 0.6643 loss_val: 1.3306 acc_val: 0.6567 time: 0.0068s\n",
      "Epoch: 0065 loss_train: 1.1544 acc_train: 0.7071 loss_val: 1.3187 acc_val: 0.6633 time: 0.0068s\n",
      "Epoch: 0066 loss_train: 1.1477 acc_train: 0.6857 loss_val: 1.3069 acc_val: 0.6700 time: 0.0068s\n",
      "Epoch: 0067 loss_train: 1.1368 acc_train: 0.7071 loss_val: 1.2953 acc_val: 0.6767 time: 0.0068s\n",
      "Epoch: 0068 loss_train: 1.1183 acc_train: 0.7214 loss_val: 1.2835 acc_val: 0.6833 time: 0.0068s\n",
      "Epoch: 0069 loss_train: 1.1272 acc_train: 0.7286 loss_val: 1.2721 acc_val: 0.6967 time: 0.0068s\n",
      "Epoch: 0070 loss_train: 1.0899 acc_train: 0.7571 loss_val: 1.2604 acc_val: 0.6933 time: 0.0078s\n",
      "Epoch: 0071 loss_train: 1.0759 acc_train: 0.7286 loss_val: 1.2497 acc_val: 0.6967 time: 0.0068s\n",
      "Epoch: 0072 loss_train: 1.1127 acc_train: 0.7286 loss_val: 1.2390 acc_val: 0.7033 time: 0.0078s\n",
      "Epoch: 0073 loss_train: 1.0629 acc_train: 0.7214 loss_val: 1.2284 acc_val: 0.7067 time: 0.0068s\n",
      "Epoch: 0074 loss_train: 1.0493 acc_train: 0.7714 loss_val: 1.2173 acc_val: 0.7200 time: 0.0068s\n",
      "Epoch: 0075 loss_train: 1.0167 acc_train: 0.8071 loss_val: 1.2057 acc_val: 0.7300 time: 0.0068s\n",
      "Epoch: 0076 loss_train: 1.0515 acc_train: 0.7500 loss_val: 1.1937 acc_val: 0.7300 time: 0.0068s\n",
      "Epoch: 0077 loss_train: 1.0132 acc_train: 0.7786 loss_val: 1.1814 acc_val: 0.7300 time: 0.0068s\n",
      "Epoch: 0078 loss_train: 0.9711 acc_train: 0.7429 loss_val: 1.1696 acc_val: 0.7367 time: 0.0068s\n",
      "Epoch: 0079 loss_train: 0.9994 acc_train: 0.7857 loss_val: 1.1579 acc_val: 0.7367 time: 0.0069s\n",
      "Epoch: 0080 loss_train: 0.9422 acc_train: 0.7857 loss_val: 1.1466 acc_val: 0.7400 time: 0.0068s\n",
      "Epoch: 0081 loss_train: 0.9560 acc_train: 0.8143 loss_val: 1.1354 acc_val: 0.7433 time: 0.0059s\n",
      "Epoch: 0082 loss_train: 1.0070 acc_train: 0.7857 loss_val: 1.1252 acc_val: 0.7500 time: 0.0068s\n",
      "Epoch: 0083 loss_train: 0.8881 acc_train: 0.7929 loss_val: 1.1156 acc_val: 0.7500 time: 0.0068s\n",
      "Epoch: 0084 loss_train: 0.9243 acc_train: 0.7929 loss_val: 1.1063 acc_val: 0.7600 time: 0.0068s\n",
      "Epoch: 0085 loss_train: 0.8859 acc_train: 0.8000 loss_val: 1.0972 acc_val: 0.7700 time: 0.0068s\n",
      "Epoch: 0086 loss_train: 0.9253 acc_train: 0.7857 loss_val: 1.0883 acc_val: 0.7733 time: 0.0068s\n",
      "Epoch: 0087 loss_train: 0.8732 acc_train: 0.8357 loss_val: 1.0801 acc_val: 0.7733 time: 0.0068s\n",
      "Epoch: 0088 loss_train: 0.8444 acc_train: 0.8571 loss_val: 1.0726 acc_val: 0.7833 time: 0.0058s\n",
      "Epoch: 0089 loss_train: 0.8496 acc_train: 0.8214 loss_val: 1.0654 acc_val: 0.7833 time: 0.0078s\n",
      "Epoch: 0090 loss_train: 0.8373 acc_train: 0.8143 loss_val: 1.0573 acc_val: 0.7800 time: 0.0059s\n",
      "Epoch: 0091 loss_train: 0.8521 acc_train: 0.8214 loss_val: 1.0475 acc_val: 0.7767 time: 0.0068s\n",
      "Epoch: 0092 loss_train: 0.8710 acc_train: 0.8143 loss_val: 1.0383 acc_val: 0.7833 time: 0.0068s\n",
      "Epoch: 0093 loss_train: 0.8302 acc_train: 0.8357 loss_val: 1.0293 acc_val: 0.7800 time: 0.0068s\n",
      "Epoch: 0094 loss_train: 0.8400 acc_train: 0.8286 loss_val: 1.0202 acc_val: 0.7833 time: 0.0059s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0095 loss_train: 0.8406 acc_train: 0.8714 loss_val: 1.0106 acc_val: 0.7900 time: 0.0068s\n",
      "Epoch: 0096 loss_train: 0.8101 acc_train: 0.8357 loss_val: 1.0014 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0097 loss_train: 0.7616 acc_train: 0.8714 loss_val: 0.9934 acc_val: 0.7900 time: 0.0068s\n",
      "Epoch: 0098 loss_train: 0.7803 acc_train: 0.8500 loss_val: 0.9867 acc_val: 0.7900 time: 0.0088s\n",
      "Epoch: 0099 loss_train: 0.7967 acc_train: 0.8071 loss_val: 0.9807 acc_val: 0.7900 time: 0.0068s\n",
      "Epoch: 0100 loss_train: 0.7344 acc_train: 0.8643 loss_val: 0.9735 acc_val: 0.7933 time: 0.0064s\n",
      "Epoch: 0101 loss_train: 0.7825 acc_train: 0.8500 loss_val: 0.9669 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0102 loss_train: 0.7703 acc_train: 0.8357 loss_val: 0.9608 acc_val: 0.7933 time: 0.0078s\n",
      "Epoch: 0103 loss_train: 0.7129 acc_train: 0.8714 loss_val: 0.9537 acc_val: 0.7933 time: 0.0059s\n",
      "Epoch: 0104 loss_train: 0.6960 acc_train: 0.8714 loss_val: 0.9467 acc_val: 0.7933 time: 0.0078s\n",
      "Epoch: 0105 loss_train: 0.7183 acc_train: 0.8500 loss_val: 0.9396 acc_val: 0.7833 time: 0.0068s\n",
      "Epoch: 0106 loss_train: 0.7019 acc_train: 0.8714 loss_val: 0.9331 acc_val: 0.7833 time: 0.0059s\n",
      "Epoch: 0107 loss_train: 0.6889 acc_train: 0.8571 loss_val: 0.9269 acc_val: 0.7900 time: 0.0068s\n",
      "Epoch: 0108 loss_train: 0.7167 acc_train: 0.8286 loss_val: 0.9202 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0109 loss_train: 0.6734 acc_train: 0.8786 loss_val: 0.9139 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0110 loss_train: 0.6812 acc_train: 0.8500 loss_val: 0.9085 acc_val: 0.7933 time: 0.0084s\n",
      "Epoch: 0111 loss_train: 0.6092 acc_train: 0.9071 loss_val: 0.9029 acc_val: 0.7900 time: 0.0064s\n",
      "Epoch: 0112 loss_train: 0.6455 acc_train: 0.9000 loss_val: 0.8988 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0113 loss_train: 0.6557 acc_train: 0.8857 loss_val: 0.8945 acc_val: 0.7867 time: 0.0073s\n",
      "Epoch: 0114 loss_train: 0.6556 acc_train: 0.8643 loss_val: 0.8903 acc_val: 0.7867 time: 0.0059s\n",
      "Epoch: 0115 loss_train: 0.6526 acc_train: 0.8571 loss_val: 0.8870 acc_val: 0.7867 time: 0.0068s\n",
      "Epoch: 0116 loss_train: 0.6163 acc_train: 0.8714 loss_val: 0.8839 acc_val: 0.7900 time: 0.0068s\n",
      "Epoch: 0117 loss_train: 0.7032 acc_train: 0.8357 loss_val: 0.8789 acc_val: 0.7900 time: 0.0068s\n",
      "Epoch: 0118 loss_train: 0.5922 acc_train: 0.9000 loss_val: 0.8727 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0119 loss_train: 0.6279 acc_train: 0.8929 loss_val: 0.8659 acc_val: 0.7967 time: 0.0059s\n",
      "Epoch: 0120 loss_train: 0.6097 acc_train: 0.9143 loss_val: 0.8608 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0121 loss_train: 0.6142 acc_train: 0.9071 loss_val: 0.8563 acc_val: 0.7967 time: 0.0088s\n",
      "Epoch: 0122 loss_train: 0.5835 acc_train: 0.8786 loss_val: 0.8530 acc_val: 0.7933 time: 0.0078s\n",
      "Epoch: 0123 loss_train: 0.6019 acc_train: 0.8857 loss_val: 0.8490 acc_val: 0.7967 time: 0.0098s\n",
      "Epoch: 0124 loss_train: 0.5878 acc_train: 0.8929 loss_val: 0.8461 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0125 loss_train: 0.5655 acc_train: 0.8929 loss_val: 0.8438 acc_val: 0.7933 time: 0.0064s\n",
      "Epoch: 0126 loss_train: 0.6166 acc_train: 0.9000 loss_val: 0.8412 acc_val: 0.7900 time: 0.0068s\n",
      "Epoch: 0127 loss_train: 0.5803 acc_train: 0.8786 loss_val: 0.8385 acc_val: 0.7900 time: 0.0068s\n",
      "Epoch: 0128 loss_train: 0.5800 acc_train: 0.9143 loss_val: 0.8345 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0129 loss_train: 0.5427 acc_train: 0.8929 loss_val: 0.8308 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0130 loss_train: 0.5663 acc_train: 0.8786 loss_val: 0.8255 acc_val: 0.7933 time: 0.0059s\n",
      "Epoch: 0131 loss_train: 0.5675 acc_train: 0.8786 loss_val: 0.8202 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0132 loss_train: 0.5813 acc_train: 0.8929 loss_val: 0.8151 acc_val: 0.7967 time: 0.0078s\n",
      "Epoch: 0133 loss_train: 0.5711 acc_train: 0.8857 loss_val: 0.8112 acc_val: 0.7967 time: 0.0059s\n",
      "Epoch: 0134 loss_train: 0.4978 acc_train: 0.9214 loss_val: 0.8080 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0135 loss_train: 0.5384 acc_train: 0.8929 loss_val: 0.8059 acc_val: 0.8000 time: 0.0068s\n",
      "Epoch: 0136 loss_train: 0.5355 acc_train: 0.9071 loss_val: 0.8026 acc_val: 0.8000 time: 0.0068s\n",
      "Epoch: 0137 loss_train: 0.5639 acc_train: 0.9071 loss_val: 0.7996 acc_val: 0.8000 time: 0.0063s\n",
      "Epoch: 0138 loss_train: 0.5932 acc_train: 0.8643 loss_val: 0.7973 acc_val: 0.8000 time: 0.0068s\n",
      "Epoch: 0139 loss_train: 0.5614 acc_train: 0.9286 loss_val: 0.7939 acc_val: 0.8000 time: 0.0069s\n",
      "Epoch: 0140 loss_train: 0.5691 acc_train: 0.8857 loss_val: 0.7915 acc_val: 0.7933 time: 0.0107s\n",
      "Epoch: 0141 loss_train: 0.4862 acc_train: 0.9286 loss_val: 0.7901 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0142 loss_train: 0.5276 acc_train: 0.8857 loss_val: 0.7887 acc_val: 0.8000 time: 0.0068s\n",
      "Epoch: 0143 loss_train: 0.5145 acc_train: 0.8929 loss_val: 0.7882 acc_val: 0.7967 time: 0.0078s\n",
      "Epoch: 0144 loss_train: 0.5158 acc_train: 0.9071 loss_val: 0.7871 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0145 loss_train: 0.5202 acc_train: 0.9214 loss_val: 0.7842 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0146 loss_train: 0.5170 acc_train: 0.9429 loss_val: 0.7800 acc_val: 0.8000 time: 0.0068s\n",
      "Epoch: 0147 loss_train: 0.4631 acc_train: 0.9143 loss_val: 0.7759 acc_val: 0.8000 time: 0.0068s\n",
      "Epoch: 0148 loss_train: 0.5039 acc_train: 0.9214 loss_val: 0.7716 acc_val: 0.7967 time: 0.0059s\n",
      "Epoch: 0149 loss_train: 0.5234 acc_train: 0.9000 loss_val: 0.7694 acc_val: 0.7967 time: 0.0059s\n",
      "Epoch: 0150 loss_train: 0.4961 acc_train: 0.9214 loss_val: 0.7678 acc_val: 0.7967 time: 0.0078s\n",
      "Epoch: 0151 loss_train: 0.5159 acc_train: 0.9071 loss_val: 0.7655 acc_val: 0.7967 time: 0.0068s\n",
      "Epoch: 0152 loss_train: 0.5101 acc_train: 0.9071 loss_val: 0.7635 acc_val: 0.7933 time: 0.0068s\n",
      "Epoch: 0153 loss_train: 0.4665 acc_train: 0.9286 loss_val: 0.7623 acc_val: 0.8000 time: 0.0068s\n",
      "Epoch: 0154 loss_train: 0.5055 acc_train: 0.9214 loss_val: 0.7617 acc_val: 0.8100 time: 0.0059s\n",
      "Epoch: 0155 loss_train: 0.4617 acc_train: 0.9286 loss_val: 0.7615 acc_val: 0.8100 time: 0.0078s\n",
      "Epoch: 0156 loss_train: 0.4717 acc_train: 0.9214 loss_val: 0.7608 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0157 loss_train: 0.5005 acc_train: 0.9214 loss_val: 0.7598 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0158 loss_train: 0.5489 acc_train: 0.8857 loss_val: 0.7564 acc_val: 0.8100 time: 0.0058s\n",
      "Epoch: 0159 loss_train: 0.4898 acc_train: 0.9357 loss_val: 0.7511 acc_val: 0.8100 time: 0.0059s\n",
      "Epoch: 0160 loss_train: 0.5165 acc_train: 0.9143 loss_val: 0.7476 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0161 loss_train: 0.4571 acc_train: 0.9143 loss_val: 0.7450 acc_val: 0.8167 time: 0.0068s\n",
      "Epoch: 0162 loss_train: 0.4581 acc_train: 0.9429 loss_val: 0.7431 acc_val: 0.8133 time: 0.0059s\n",
      "Epoch: 0163 loss_train: 0.4777 acc_train: 0.9357 loss_val: 0.7411 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0164 loss_train: 0.4667 acc_train: 0.9143 loss_val: 0.7393 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0165 loss_train: 0.4697 acc_train: 0.9286 loss_val: 0.7369 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0166 loss_train: 0.4284 acc_train: 0.9357 loss_val: 0.7349 acc_val: 0.8133 time: 0.0058s\n",
      "Epoch: 0167 loss_train: 0.4653 acc_train: 0.9214 loss_val: 0.7333 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0168 loss_train: 0.4479 acc_train: 0.9143 loss_val: 0.7328 acc_val: 0.8133 time: 0.0068s\n",
      "Epoch: 0169 loss_train: 0.4203 acc_train: 0.9429 loss_val: 0.7331 acc_val: 0.8133 time: 0.0068s\n",
      "Epoch: 0170 loss_train: 0.4813 acc_train: 0.9214 loss_val: 0.7335 acc_val: 0.8133 time: 0.0068s\n",
      "Epoch: 0171 loss_train: 0.4089 acc_train: 0.9429 loss_val: 0.7326 acc_val: 0.8133 time: 0.0059s\n",
      "Epoch: 0172 loss_train: 0.4208 acc_train: 0.9500 loss_val: 0.7317 acc_val: 0.8167 time: 0.0068s\n",
      "Epoch: 0173 loss_train: 0.4273 acc_train: 0.9357 loss_val: 0.7302 acc_val: 0.8100 time: 0.0058s\n",
      "Epoch: 0174 loss_train: 0.4364 acc_train: 0.9143 loss_val: 0.7278 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0175 loss_train: 0.3988 acc_train: 0.9429 loss_val: 0.7248 acc_val: 0.8133 time: 0.0068s\n",
      "Epoch: 0176 loss_train: 0.4974 acc_train: 0.8786 loss_val: 0.7222 acc_val: 0.8133 time: 0.0059s\n",
      "Epoch: 0177 loss_train: 0.4219 acc_train: 0.9357 loss_val: 0.7194 acc_val: 0.8133 time: 0.0068s\n",
      "Epoch: 0178 loss_train: 0.4316 acc_train: 0.9286 loss_val: 0.7176 acc_val: 0.8133 time: 0.0068s\n",
      "Epoch: 0179 loss_train: 0.4347 acc_train: 0.9214 loss_val: 0.7171 acc_val: 0.8133 time: 0.0059s\n",
      "Epoch: 0180 loss_train: 0.3620 acc_train: 0.9571 loss_val: 0.7170 acc_val: 0.8167 time: 0.0068s\n",
      "Epoch: 0181 loss_train: 0.4115 acc_train: 0.9357 loss_val: 0.7186 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0182 loss_train: 0.4330 acc_train: 0.9429 loss_val: 0.7191 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0183 loss_train: 0.4398 acc_train: 0.9571 loss_val: 0.7208 acc_val: 0.8100 time: 0.0059s\n",
      "Epoch: 0184 loss_train: 0.3791 acc_train: 0.9500 loss_val: 0.7211 acc_val: 0.8100 time: 0.0059s\n",
      "Epoch: 0185 loss_train: 0.4255 acc_train: 0.9571 loss_val: 0.7192 acc_val: 0.8167 time: 0.0069s\n",
      "Epoch: 0186 loss_train: 0.4449 acc_train: 0.9357 loss_val: 0.7176 acc_val: 0.8200 time: 0.0068s\n",
      "Epoch: 0187 loss_train: 0.4354 acc_train: 0.9357 loss_val: 0.7146 acc_val: 0.8100 time: 0.0059s\n",
      "Epoch: 0188 loss_train: 0.4031 acc_train: 0.9500 loss_val: 0.7126 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0189 loss_train: 0.3970 acc_train: 0.9429 loss_val: 0.7106 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0190 loss_train: 0.4191 acc_train: 0.9357 loss_val: 0.7084 acc_val: 0.8100 time: 0.0063s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0191 loss_train: 0.4250 acc_train: 0.9571 loss_val: 0.7061 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0192 loss_train: 0.4002 acc_train: 0.9571 loss_val: 0.7051 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0193 loss_train: 0.4082 acc_train: 0.9286 loss_val: 0.7059 acc_val: 0.8133 time: 0.0059s\n",
      "Epoch: 0194 loss_train: 0.3701 acc_train: 0.9214 loss_val: 0.7082 acc_val: 0.8167 time: 0.0059s\n",
      "Epoch: 0195 loss_train: 0.3871 acc_train: 0.9571 loss_val: 0.7115 acc_val: 0.8100 time: 0.0068s\n",
      "Epoch: 0196 loss_train: 0.3849 acc_train: 0.9571 loss_val: 0.7125 acc_val: 0.8067 time: 0.0068s\n",
      "Epoch: 0197 loss_train: 0.4187 acc_train: 0.9286 loss_val: 0.7109 acc_val: 0.8067 time: 0.0068s\n",
      "Epoch: 0198 loss_train: 0.3656 acc_train: 0.9500 loss_val: 0.7073 acc_val: 0.8100 time: 0.0069s\n",
      "Epoch: 0199 loss_train: 0.3828 acc_train: 0.9429 loss_val: 0.7044 acc_val: 0.8100 time: 0.0059s\n",
      "Epoch: 0200 loss_train: 0.4004 acc_train: 0.9286 loss_val: 0.7007 acc_val: 0.8100 time: 0.0059s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.3870s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T12:28:28.271361Z",
     "start_time": "2021-02-27T12:28:28.264019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7228 accuracy= 0.8380\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features,adj)\n",
    "    loss_test = F.nll_loss(output[idx_test],labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test],labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
