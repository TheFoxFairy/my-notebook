# 神经网络基础

## 介绍

神经网络的运行过程分为三步： 前向传播、 反向传播、 参数更新， 通过不断迭代进行模型参数的更新， 以从数据中挖掘出有价值的信。

![image-20210227210221535](assets/02-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/image-20210227210221535.png)

1. 前向传播： 给定输入和参数， 逐层向前进行计算， 最后输出预测结果；
2. 反向传播： 基于前向传播得到的预测结果， 使用损失函数得到损失值， 然后计算
   相关参数的梯度， 该计算方法称为反向传播（back-propagation） ， 具体的细节后面将详细
   介绍；
3. 参数更新： 使用梯度下降算法对参数进行更新， 重复上述过程， 逐步迭代， 直到
   模型收敛  。

## 反向传播

以多层感知器为例子，进行介绍反向传播算法。给定样本$\{(x_n,y_n)\}_{n=1}^N$，使用多层感知器的消息传递公式可以进行前向传播，这个过程用下进行描述：

![image-20210228103424582](assets/02-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/image-20210228103424582.png)

给定样本$(x,y)$，前向传播得到输出$\hat y$，对应的损失值为$L(y,\hat y)$，接下来求参数矩阵$W^{(l)}$的梯度$\frac{\partial{L(y,\hat y)}}{\partial{W^{(l)}}}$，使用链式法则如下：
$$
\frac{\partial{L(y,\hat y)}}{\partial{W^{(l)}}} = \frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}}\frac{\partial{z^{(l)}}}{\partial{W^{(l)}}}
$$
定义$\frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}}$为误差项，它衡量的是对$z^{(l)}$对损失值的影响，进一步使用链式法则，可以得到：
$$
\delta^{(l)} = \frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}} = 
	\frac{\partial{a^{(l)}}}{\partial{z^{(l)}}} \times
	\frac{\partial{z^{(l+1)}}}{\partial{a^{(l)}}} \times
	\frac{\partial{L^{(y,\hat y)}}}{\partial{z^{(l+1)}}}
$$
**由于$z^{(l+1)} = W^{(l+1)}a^{(l)} + b^{(l)}$且$a^{(l)}=\sigma(z^{(l)})$进行变换可以得到：**
$$
\delta^{(l)} = \frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}} = 
	\frac{\partial{a^{(l)}}}{\partial{z^{(l)}}} \times
	\frac{\partial{z^{(l+1)}}}{\partial{a^{(l)}}} \times
	\frac{\partial{L^{(y,\hat y)}}}{\partial{z^{(l+1)}}}=\sigma^{'}(z^{(l)})\odot W^{(l+1)^T}\delta^{(l+1)}
$$
其中，$\sigma^{'}(z^{(l)})$是激活函数的导数，$\odot$表达哈达玛积，是一种对应元素相乘的二元运算符。也就是：
$$
\begin{align}
\because \delta_j^{(l)} &= \frac{\part{L{(y,\hat y)}}}{\part{z_j^{(l)}}} = \frac{\part{L{(y,\hat y)}}}{\part{a_j^{(l)}}}\cdot  \frac{\part{a_j^{(l)}}}{\part{z_j^{(l)}}}  \\

\therefore \delta^{(l)} &= \frac{\part{L{(y,\hat y)}}}{\part{a^{(l)}}}\odot  \frac{\part{a^{(l)}}}{\part{z^{(l)}}} = \nabla_a L(y,\hat y)\odot\sigma^{'}(z^{(l)})  \\
\end{align}
$$
从上可以看出，第一层的误差与第${l+1}$层的误差有关，这就是反向传播的来源。

对于$\frac{\part{L(y,\hat y)}}{\part{W^{(l)}}} \in R^{(D_l\times D_{l-1})}$，有
$$
\frac{\partial{L(y,\hat y)}}{\partial{W^{(l)}}} = \frac{\partial{L(y,\hat y)}}{\partial{z^{(l)}}}\frac{\partial{z^{(l)}}}{\partial{W^{(l)}}}=(a^{(l-1)}\delta^{(l)^T})^T 
$$
偏置项$b^{(l)}$的导数：
$$
\frac{\part{L(y,\hat y)}}{\part b^{(l)}} = \delta^{(l)}
$$

## 梯度消失

对于深度神经网络来说，虽然反向传播能够高效地计算梯度，但是随着堆叠层数和模型参数规模的增加，也给模型优化带来一些严峻的问题。

从上面的式子中可以看出，第$l$层的误差是通过第$l+1$层的误差与两层之间权重的加权，再乘以激活函数的导数得到的，如果激活函数使用`Sigmoid`，它的导数为$σ'(x)=σ(x)(1–σ(x))$，由于$σ(x)\in(0,1)$，它的导数的最大值为$σ'(x)=0.25$，当层数增加时，最后一层的误差将在前面的层中快速衰减，这会导致靠近输入层的梯度值非常小，参数几乎无法进行有效的更新，在下一次前向传播时，由于前面层的参数无法有效地从数据中获得有价值的信息供后面的层使用，模型就难以进行有效的训练。这种现象称为梯度消失。

导致梯度消失的原因在于激活函数的饱和性，比如`Sigmoid`、`Tanh`等都会带来这种问题，它们在函数值趋近于上下边界时，梯度通常比较小，再与误差项相乘将变得更小，多次的反向传播将会使得梯度值不断减小。

因此现在的神经网络通常使用`ReLU`激活函数以及其变种。

##  深度学习基础

